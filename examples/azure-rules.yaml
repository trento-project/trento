---
controls:
version: 1.0.0
id: 1
description: "HA Checker for SAP - Azure Generic Checks"
type: "master"
groups:
  - id: 1.1
    description: "Corosync checks for Azure"
    checks:
      - id: 1.1.1
        description: "Corosync Token is set to 30000"
        audit: 'grep -E "^\s*token:\s*30000" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 30000
        remediation: |
          Adjust the corosync tokem timeout as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker 
        scored: true
      - id: 1.1.1.runtime
        description: "Corosync Token is set to 30000, check it runtime"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.token (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 30000
        remediation: |
          Adjust the corosync tokem timeout as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.2
        description: "Corosync consensus is set to 36000"
        audit: 'grep -E "^\s*consensus:\s*36000" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 36000
        remediation: |
          Adjust the corosync consensus timeout as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker 
        scored: true
      - id: 1.1.2.runtime
        description: "Corosync consensus is set to 36000"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.consensus (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 36000
        remediation: |
          Adjust the corosync consensus timeout as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.3
        description: "Corosync max_messages is set to 20"
        audit: 'grep -E "^\s*max_messages:\s*20" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 20
        remediation: |
          Adjust the corosync max_messages parameter as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker 
        scored: true
      - id: 1.1.3.runtime
        description: "Corosync max_messages is set to 20, check it runtime"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.max_messages (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 20
        remediation: |
          Adjust the corosync max_messages parameter as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.4
        description: "Corosync join parameter is set to 60"
        audit: 'grep -E "^\s*join:\s*60" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 60
        remediation: |
          Adjust the corosync join parameter as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker 
        scored: true
      - id: 1.1.4.runtime
        description: "Corosync join parameter is set to 60, check it runtime"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.join (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 60
        remediation: |
          Adjust the corosync join parameter as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.5
        description: "Corosync token_retransmits_before_loss_const parameter is set to 10"
        audit: 'grep -E "^\s*token_retransmits_before_loss_const:\s*10" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 10
        remediation: |
          Adjust the corosync token_retransmits_before_loss_const parameter as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker 
        scored: true
      - id: 1.1.5.runtime
        description: "Corosync token_retransmits_before_loss_const parameter is set to 10, check it runtime"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.token_retransmits_before_loss_const (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 10
        remediation: |
          Adjust the corosync token_retransmits_before_loss_const parameter as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.6
        description: "Corosync transport parameter is set to udpu"
        audit: 'grep -E "^\s*transport:\s*udpu" /etc/corosync/corosync.conf | sed "s/.*:\s*//"'
        tests:
          test_items:
            - flag: "udpu"
        remediation: |
          Adjust the corosync transport parameter as recommended on the Azure best practices. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker 
        scored: true
      - id: 1.1.7
        description: "Corosync expected_votes parameter is set to 2"
        audit: 'grep -E "^\s*expected_votes:\s*2" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 2
        remediation: |
          Adjust the corosync expected_votes parameter to 2 to make sure pacemaker calculates the actions properly for a two-node cluster. 
          Check the Azure best practices for more information. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker 
        scored: true
      - id: 1.1.8
        description: "Corosync two_node parameter is set to 1"
        audit: 'grep -E "^\s*two_node:\s*1" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 1
        remediation: |
          Adjust the corosync two_node parameter to 1 to make sure pacemaker calculates the actions properly for a two-node cluster. 
          Check the Azure best practices for more information. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker 
        scored: true
      - id: 1.1.8.runtime
        description: "Corosync two_node parameter is set to 1, check it runtime"
        audit: 'corosync-cmapctl | grep "runtime.votequorum.two_node (u8) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 1
        remediation: |
          Adjust the corosync two_node parameter to 1 to make sure pacemaker calculates the actions properly for a two-node cluster.
          Check the Azure best practices for more information. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
  - id: 1.2
    description: "Pacemaker checks for Azure"
    checks:
      - id: 1.2.1
        description: "Fencing is enabled"
        audit: "crm_attribute -t crm_config -G -n stonith-enabled --quiet"
        tests:
          test_items:
            - flag: 'true'
        remediation: |
          Fencing is mandatory to guarantee data integrity for your SAP Applications. Disabling this brings your environment into an unsupported state.
          
          Execute the following command to enable it:

            # crm configure property stonith-enabled=true 
        scored: true
      - id: 1.2.2
        description: "Fencing timeout is correctly configured"
        audit: |
          timeout=$(crm_attribute -t crm_config -G -n stonith-timeout --quiet)
          if cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/@type" > /dev/null 2>&1; then
            if [[ ${timeout} = "900" ]]; then
              echo "correct"
            else
              echo ${timeout}
            fi
          elif [[ ${timeout} = "144" ]]; then
            echo "correct"
          else
            echo ${timeout}  
          fi
        tests:
          test_items:
            - flag: "correct"
        remediation: |
          The fencing timeout (stonith-timeout) determines the time pacemaker will wait fencing to succeed. 
          The recommended values on Azure for SBD only fencing is 144s and 900s when using SBD combined with Azure Fence agent.
          Check the Azure best practices for more information. https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker

          Execute the following command to adjust the timeout for your usecase:

            # crm configure property stonith-timeout={144|900} 
        scored: true
  - id: 1.3
    description: "SBD checks for Azure"
    checks:
      - id: 1.3.1
        description: "Pacemaker and SBD integration is enabled"
        audit: "cat /etc/sysconfig/sbd | grep SBD_PACEMAKER"
        tests:
          test_items:
            - flag: "SBD_PACEMAKER"
              compare:
                op: eq
                value: "yes"
        remediation: |
          For proper SBD fencing, make sure that the integration with Pacemaker is enabled. 

          IMPORTANT: Always test these steps in your QA environment before executing in productive environments!
          
          To ajust the configuration:
           
          1. Run the following commands to put cluster into maintenance mode:
          
             # crm configure property maintenance-mode=true

          2. Stop the cluster:

             # crm cluster stop   
          
          2. Set the SBD_STARTMODE parameter to "yes" on /etc/sysconfig/sbd:
             
               [...]
               SBD_PACEMAKER="yes"
               [...]
          
          3. Restart the cluster: 
             
             # crm cluster start

          4. Put cluster out of maintenance mode 

             # crm configure property maintenance-mode=false
        scored: true
      - id: 1.3.2
        description: "SBD start mode is set to always"
        audit: "cat /etc/sysconfig/sbd | grep SBD_STARTMODE"
        tests:
          test_items:
            - flag: "SBD_STARTMODE"
              compare:
                op: eq
                value: "always"
        remediation: |
          If not set to always, SBD will not automatically start if the node was previously fenced as it will spect the cluster in a clean state. 
          
          IMPORTANT: Always test these steps in your QA environment before executing in productive environments!
          
          To ajust the configuration:
           
          1. Run the following commands to put cluster into maintenance mode:
          
             # crm configure property maintenance-mode=true

          2. Stop the cluster:

             # crm cluster stop   
          
          2. Set the SBD_STARTMODE parameter to "always" on /etc/sysconfig/sbd:
             
               [...]
               SBD_STARTMODE="always"
               [...]
          
          3. Restart the cluster: 
             
             # crm cluster start

          4. Put cluster out of maintenance mode 

             # crm configure property maintenance-mode=false
        scored: true
      - id: 1.3.3
        description: "SBD service is enabled"
        audit: "systemctl is-enabled sbd"
        tests:
          test_items:
            - flag: "enabled"
        remediation: |
          If not enabled, SBD service will not start automatically after reboots, affecting the correct cluster startup. 
          
          To ajust the configuration, run:
           
            # systemctl enable sbd
        scored: true
      - id: 1.3.4
        description: "Multiple SBD devices are configured"
        audit: |
          sbdarray=$(grep -E '^SBD_DEVICE=' /etc/sysconfig/sbd  | grep -oP 'SBD_DEVICE=\K[^.]+' | sed 's/\"//g')
          IFS=';' sbdarray=( $sbdarray )
          echo "SBD_COUNT=${#sbdarray[@]}"
        tests:
          test_items:
            - flag: "SBD_COUNT"
              compare:
                op: gte
                value: 2
        remediation: |
          It is recommended to configure up to 3 SBD devices for production environments.
          
          Check more information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#set-up-sbd-device
        scored: false
      - id: 1.3.5
        description: "SBD service is enabled"
        audit: "systemctl is-enabled sbd"
        tests:
          test_items:
            - flag: "enabled"
        remediation: |
          If not enabled, SBD service will not start automatically after reboots, affecting the correct cluster startup. 
          
          To ajust the configuration, run:
           
            # systemctl enable sbd
        scored: true
      - id: 1.3.6
        description: "SBD Timing parameters configured properly"
        audit: |
          DEF_WDTIMEOUT=60
          DEF_MSGWAIT=120
          result_wdtimeout=${DEF_TIMEOUT}
          result_msgwait=${DEF_MSGWAIT}
         
          sbdarray=$(grep -E '^SBD_DEVICE=' /etc/sysconfig/sbd  | grep -oP 'SBD_DEVICE=\K[^.]+' | sed 's/\"//g')
          IFS=';' sbdarray=( $sbdarray )

          for i in "${sbdarray[@]}"
          do
            msgwait=$(/usr/sbin/sbd -d ${i} dump | grep -oP 'Timeout \(msgwait\)  *: \K\d+')
            wdtimeout=$(/usr/sbin/sbd -d ${i} dump | grep -oP 'Timeout \(watchdog\)  *: \K\d+')

            if [[ "${msgwait}" -ne "${DEF_MSGWAIT}" ]]; then
              result_msgwait="${msgwait}"
            fi  

            if [[ "${wdtimeout}" -ne "${DEF_WDTIMEOUT}" ]]; then
              result_wdtimeout="${wdtimeout}"
            fi  
          done
          echo "${result_wdtimeout}" 
          echo "${result_msgwait}" 
        tests:
          test_items:
            - flag: "60"
            - flag: "120"
        remediation: |
          Make sure you configure your SBD Watchdog Timeout to 60s and the SBD msgwait to 120s as recommended on the best practices.
          
          Check more information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#set-up-sbd-device
        scored: true
  - id: 1.4
    description: "Azure Fence Agent Checks"
    checks:
      - id: 1.4.1
        description: "Azure Fence Agent retries and timeout parameters check (in case its configured)"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/@type" > /dev/null 2>&1; then
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='pcmk_monitor_retries']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='pcmk_action_limit']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='power_timeout']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='pcmk_reboot_timeout']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/operations/op [@name='monitor']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/operations/op [@name='monitor']" | grep -oP 'timeout="\K[^"]+' 
          else
            # Azure Fence Agent is not configured. We echo expected values just to fulfill the check
            echo 4
            echo 3
            echo 240
            echo 900
            echo 3600
            echo 120
          fi       
          
        tests:
          bin_op: and
          test_items:
            - flag: 4
            - flag: 3
            - flag: 240
            - flag: 900
            - flag: 3600
            - flag: 120
        remediation: |
          The Azure SAP Clustering best practices determines specific timing and retries parameter values for proper functioning of the Azure Fence agent.
          Please, make sure to set the following values for the fence-agent parameter:
             
             pcmk_monitor_retries=4
             pcmk_action_limit=3 
             power_timeout=240 
             pcmk_reboot_timeout=900 

           and the following monitor operation configuration:
             
             interval=3600
             timeout=120
           
           More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#1-create-the-stonith-devices
        scored: false
      - id: 1.4.2
        description: "Concurrent fencing should be enabled if Azure Fence is configured"
        audit: |
          cf_status=$(crm_attribute -t crm_config -G -n concurrent-fencing --quiet)

          if cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/@type" > /dev/null 2>&1; then
            if [[ ${cf_status} = "true" ]]; then
              echo "correct"
            else
              echo "${cf_status}"
            fi
          else
            echo "fence_azure_arm not configured"
          fi
        tests:
          bin_op: or
          test_items:
            - flag: "correct"
            - flag: "fence_azure_arm not configured"
        remediation: |
          Concurrent fencing must be enabled in order to use Azure Fence agent combined with SBD fencing.
          To enable it, run:
           
            # crm configure property concurrent-fencing=true
          
          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#1-create-the-stonith-devices
        scored: true
  - id: 1.5
    description: "SAP HANA System Replication Resource Agent Checks - Performance Optimized"
    checks:
      - id: 1.5.1
        description: "Cluster default resource-stickiness and migration-threshold properly configured"
        audit: |
          crm_attribute -t rsc_defaults -G -n resource-stickiness --quiet
          crm_attribute -t rsc_defaults -G -n migration-threshold --quiet
        tests:
          bin_op: and
          test_items:
            - flag: 1000
            - flag: 5000
        remediation: |
          For proper SAP HANA cluster actions calculations, it is needed to set resource-stickiness=1000 and migration-threshold=5000.
          
          Execute the following command to enable it:

            # crm configure rsc_defaults resource-stickiness=1000
            # crm configure rsc_defaults migration-threshold=5000
          
          More info at: https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability
        scored: true
      - id: 1.5.2
        description: "SAP HANA Topology Resource Agent properly configured"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/@type" > /dev/null 2>&1; then
             
            is_clone=$(cibadmin -Q --xpath "//clone/primitive[@type='SAPHanaTopology']/@type" > /dev/null 2>&1; echo $?)

            if [[ "${is_clone}" = "0" ]]; then
              echo "is_clone"
            else 
              echo "not_clone"
            fi

            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='monitor']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='monitor']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='start']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='start']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='stop']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='stop']" | grep -oP 'timeout="\K[^"]+'
          else
            # SAPHanaTopology is not configured
            echo "SAPHanaTopology is not configured"
          fi
        tests:
          bin_op: and
          test_items:
            - flag: "is_clone"
            - flag: 10
            - flag: 600
            - flag: 0
            - flag: 600
            - flag: 0
            - flag: 300
        remediation: |
          The SAPHanaTopology resource-agent is responsible to monitor the status of the SAP HANA System Replication and provide this information for actions calculations by Pacemaker and the SAPHanaSR resource-agent
          
          Make sure that it is configured as a clone resource, so it runs on all nodes of the cluster and that the following timings are set for the operations:
            
            monitor: interval="10" timeout="600" 
            start:  interval="0" timeout="600"
            stop: interval="0" timeout="300"

          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#create-sap-hana-cluster-resources
        scored: true
      - id: 1.5.3
        description: "SAP HANA Resource Agent properly configured"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='SAPHana']/@type" > /dev/null 2>&1; then
             
            is_msl=$(cibadmin -Q --xpath "//master/primitive[@type='SAPHana']/@type" > /dev/null 2>&1; echo $?)

            if [[ "${is_msl}" = "0" ]]; then
              echo "is_msl"
            else 
              echo "not_msl"
            fi

            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='monitor'] [@role='Master']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='monitor'] [@role='Master']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='monitor'] [@role='Slave']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='monitor'] [@role='Slave']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='start']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='start']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='stop']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='stop']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='promote']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='promote']" | grep -oP 'timeout="\K[^"]+'

            cibadmin -Q --xpath "//primitive[@type='SAPHana']/instance_attributes/nvpair [@name='PREFER_SITE_TAKEOVER']" | grep -oP 'value="\K[^"]+' | tr '[:upper:]' '[:lower:]'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/instance_attributes/nvpair [@name='AUTOMATED_REGISTER']" | grep -oP 'value="\K[^"]+' | tr '[:upper:]' '[:lower:]'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/instance_attributes/nvpair [@name='DUPLICATE_PRIMARY_TIMEOUT']" | grep -oP 'value="\K[^"]+'
          else
            # SAPHana is not configured
            echo "SAPHana RA is not configured"
          fi
        tests:
          bin_op: and
          test_items:
            - flag: "is_msl"
            - flag: 60
            - flag: 700
            - flag: 61
            - flag: 700
            - flag: 0
            - flag: 3600
            - flag: 0
            - flag: 3600
            - flag: 0
            - flag: 3600
            - flag: "true"
            - flag: "false"
            - flag: 7200
        remediation: |
          The SAPHana resource-agent is responsible to perform SAP HANA actions like start, stop and take-over based on the Pacemaker calculated transitions.
          
          Make sure that it is configured as a Promoted/Demoted (Master/Slave) resource, and that the following timings are set for the operations:
            
            monitor (master role): interval="60" timeout="700"
            monitor (slave role): interval="61" timeout="700" 
            start:  interval="0" timeout="3600"
            stop: interval="0" timeout="3600"
            promote: interval="0" timeout="3600"

          And that the following input parameters are properly set:
            
            PREFER_SITE_TAKEOVER="true"
            DUPLICATE_PRIMARY_TIMEOUT="7200"
            AUTOMATED_REGISTER="false"

          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#create-sap-hana-cluster-resources
        scored: true
      - id: 1.5.4
        description: "Virtual IP Resource Agent properly configured"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='IPaddr2']/@type" > /dev/null 2>&1; then
             
            is_grouped=$(cibadmin -Q --xpath "//group/primitive[@type='IPaddr2']/@type" > /dev/null 2>&1; echo $?)

            if [[ "${is_grouped}" = "0" ]]; then
              echo "is_grouped"
            else 
              echo "not_grouped"
            fi

            cibadmin -Q --xpath "//primitive[@type='IPaddr2']/operations/op [@name='monitor']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='IPaddr2']/operations/op [@name='monitor']" | grep -oP 'timeout="\K[^"]+'
          else
            # Virtual IP is not configured
            echo "vIP RA is not configured"
          fi
        tests:
          bin_op: and
          test_items:
            - flag: "is_grouped"
            - flag: 10
            - flag: 20
        remediation: |
          The IPaddr2 resource-agent is responsible to manage the floating virtual IP used to acccess the SAP HANA and to reconfigure it in case of take-overs. 
          
          Make sure that it is configured and grouped together with the azure-lb resource agent, and that the following timings are set for the operations:
            
            monitor: interval="10" timeout="20"

          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#create-sap-hana-cluster-resources
        scored: true
      - id: 1.5.5
        description: "Azure Load Balancer Resource Agent properly configured"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='azure-lb']/@type" > /dev/null 2>&1; then
             
            is_grouped=$(cibadmin -Q --xpath "//group/primitive[@type='azure-lb']/@type" > /dev/null 2>&1; echo $?)

            if [[ "${is_grouped}" = "0" ]]; then
              echo "is_grouped"
            else 
              echo "not_grouped"
            fi

            cibadmin -Q --xpath "//primitive[@type='azure-lb']/meta_attributes/nvpair [@name='resource-stickiness']" | grep -oP 'value="\K[^"]+'

          else
            # azure-lb RA is not configured
            echo "azure-lb RA is not configured"
          fi
        tests:
          bin_op: and
          test_items:
            - flag: "is_grouped"
            - flag: 0
        remediation: |
          The azure-lb resource-agent is responsible to provide health probes to inform the Azure Load Balancer if a switch to secorary node is needed.

          Make sure that it is configured and grouped together with the IPaddr2 resource and that the resource-stickiness is set to 0.
          This configuration guarantees that the cluster migrate the azure-lb together with the SAP HANA primaru resource in case of a failover.
           
          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#create-sap-hana-cluster-resources
        scored: true
      - id: 1.5.6
        description: "Colocation and Order Constraints properly configured"
        audit: |
          if cibadmin -Q --xpath "//constraints/rsc_colocation" > /dev/null 2>&1; then
            cibadmin -Q --xpath "//constraints/rsc_colocation" | grep -oP 'score="\K[^"]+'
            cibadmin -Q --xpath "//constraints/rsc_colocation" | grep -oP 'rsc="\K[^"]+' | grep -o g_ip
            cibadmin -Q --xpath "//constraints/rsc_colocation" | grep -oP 'rsc-role="\K[^"]+'
            cibadmin -Q --xpath "//constraints/rsc_colocation" | grep -oP 'with-rsc="\K[^"]+' | grep -o msl_SAPHana
            cibadmin -Q --xpath "//constraints/rsc_colocation" | grep -oP 'with-rsc-role="\K[^"]+'
          else
            # Colocation constraint is not configured
            echo "colocation constraint is not configured"
          fi
          if cibadmin -Q --xpath "//constraints/rsc_order" > /dev/null 2>&1; then
            cibadmin -Q --xpath "//constraints/rsc_order" | grep -oP 'kind="\K[^"]+'
            cibadmin -Q --xpath "//constraints/rsc_order" | grep -oP 'first="\K[^"]+' | grep -o cln_SAPHanaTopology
            cibadmin -Q --xpath "//constraints/rsc_order" | grep -oP 'then="\K[^"]+' | grep -o msl_SAPHana
          else
            # Colocation constraint is not configured
            echo "order constraint is not configured"
          fi


        tests:
          bin_op: and
          test_items:
            - flag: 4000
            - flag: "g_ip"
            - flag: "Started"
            - flag: "msl_SAPHana"
            - flag: "Master"
            - flag: "Optional"
            - flag: "cln_SAPHanaTopology"
            - flag: "msl_SAPHana"
        remediation: |
          An constraint enforcing the group of resources containing the vIP and azurel-lb resources to stay always together with the SAP HANA resource is needed to guarantee that the SAP HANA database will still be accessible in case of failover.
          Also, make sure that the constraing score is set to 4000 to guarantee proper actions calculations related with the defaults set for resource-stickiness=1000.

          An order constraint is also needed to guarantee that SAPHanaTopology resource starts before the SAPHana resource, avoiding miscalculations regarding the cluster status. 

          IMPORTANT: This check is based on the resource prefix names recommended on the documentation (e.g. "g_ip_*", "msl_SAPHana_*", "cln_SAPHanaTopology_*"). 
          Please, check the naming convention in case the resource configuration is done correctly.
           
          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#create-sap-hana-cluster-resources
        scored: true