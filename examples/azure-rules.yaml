
controls:
version: 1.0.0
id: 1
description: "HA Configuration checks for SAP on MS Azure (generic scenario)"
type: "master"
groups:
  - id: 1.1
    description: "Corosync"
    checks:
      - id: 1.1.1
        description: "Corosync Token is set to 30000"
        audit: 'grep -E "^\s*token:\s*30000" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 30000
        remediation: |
          ## Remediation
          Adjust the Corosync `token` timeout as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.1.runtime
        description: "Corosync Token is set to 30000 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.token (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 30000
        remediation: |
          ## Abstract
          The runtime value of the Corosync `token` timeout is not set as recommended.

          ## Remediation
          Adjust the corosync `token` timeout as recommended on the Azure best practices, and reload the corosync service

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.2
        description: "Corosync consensus is set to 36000"
        audit: 'grep -E "^\s*consensus:\s*36000" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 36000
        remediation: |
          ## Remediation
          Adjust the Corosync `consensus` timeout as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.2.runtime
        description: "Corosync consensus is set to 36000 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.consensus (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 36000
        remediation: |
          ## Abstract
          The runtime value of the Corosync `consensus` timeout is not set as recommended.

          ## Remediation
          Adjust the corosync `consensus` timeout as recommended on the Azure best practices, and reload the corosync service.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.3
        description: "Corosync max_messages is set to 20"
        audit: 'grep -E "^\s*max_messages:\s*20" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 20
        remediation: |
          ## Remediation
          Adjust the Corosync `max_messages` parameter as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.3.runtime
        description: "Corosync max_messages is set to 20 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.max_messages (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 20
        remediation: |
          ## Abstract
          The runtime value of the Corosync `max_messages` parameter is not set as recommended.

          ## Remediation
          Adjust the corosync `max_messages` parameter as recommended on the Azure best practices, and reload the corosync service.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.4
        description: "Corosync join parameter is set to 60"
        audit: 'grep -E "^\s*join:\s*60" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 60
        remediation: |
          ## Abstract
          The `max_messages` Corosync configuration parameter is not set as recommended.
          
          ## Remediation
          Adjust the Corosync `join` parameter as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.4.runtime
        description: "Corosync join parameter is set to 60 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.join (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 60
        remediation: |
          ## Abstract
          The runtime value of the Corosync `join` parameter is not set as recommended.

          ## Remediation
          Adjust the corosync `join` parameter as recommended on the Azure best practices, and reload the corosync service.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.5
        description: "Corosync token_retransmits_before_loss_const parameter is set to 10"
        audit: 'grep -E "^\s*token_retransmits_before_loss_const:\s*10" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 10
        remediation: |
          ## Remediation
          Adjust the corosync `token_retransmits_before_loss_const` parameter as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.5.runtime
        description: "Corosync token_retransmits_before_loss_const parameter is set to 10 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.token_retransmits_before_loss_const (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 10
        remediation: |
          ## Abstract
          The runtime value of the corosync `token_retransmits_before_loss_const` parameter is not set as recommended

          ## Remediation
          Adjust the corosync `token_retransmits_before_loss_const` parameter as recommended on the Azure best practices, and reload the corosync service.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.6
        description: "Corosync transport parameter is set to udpu"
        audit: 'grep -E "^\s*transport:\s*udpu" /etc/corosync/corosync.conf | sed "s/.*:\s*//"'
        tests:
          test_items:
            - flag: "udpu"
        remediation: |
          ## Remediation
          Adjust the corosync transport parameter as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.7
        description: "Corosync expected_votes parameter is set to 2"
        audit: 'grep -E "^\s*expected_votes:\s*2" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 2
        remediation: |
          ## Remediation
          Adjust the corosync `expected_votes` parameter to `2` to make sure pacemaker calculates the actions properly for a two-node cluster.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.8
        description: "Corosync two_node parameter is set to 1"
        audit: 'grep -E "^\s*two_node:\s*1" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 1
        remediation: |
          ## Abstract
          The runtime value of the corosync `two_node` parameter is not set as recommended.

          ## Remediation
          Adjust the corosync two_node parameter to `1` to make sure Pacemaker calculates the actions properly for a two-node cluster.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.8.runtime
        description: "Corosync two_node parameter is set to 1 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.votequorum.two_node (u8) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 1
        remediation: |
          ## Abstract
          The runtime value of the corosync `two_node` parameter is not set as recommended.

          ## Remediation
          Adjust the corosync `two_node` parameter to `1` to make sure Pacemaker calculates the actions properly for a two-node cluster, and reload the Corosync service.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
  - id: 1.2
    description: "Pacemaker"
    checks:
      - id: 1.2.1
        description: "Fencing is enabled"
        audit: "crm_attribute -t crm_config -G -n stonith-enabled --quiet"
        tests:
          test_items:
            - flag: 'true'
        remediation: |
          ## Abstract
          Fencing is mandatory to guarantee data integrity for your SAP Applications.
          Running a HA Cluster without fencing is not supported and might cause data loss.

          ## Remediation
          Execute the following command to enable it:
          ```
          crm configure property stonith-enabled=true
          ```

          ## References
          - T.B.D.
        scored: true
      - id: 1.2.2
        description: "Fencing timeout is correctly configured"
        audit: |
          timeout=$(crm_attribute -t crm_config -G -n stonith-timeout --quiet)
          if cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/@type" > /dev/null 2>&1; then
            if [[ ${timeout} = "900" ]]; then
              echo "correct"
            else
              echo ${timeout}
            fi
          elif [[ ${timeout} = "144" ]]; then
            echo "correct"
          else
            echo ${timeout}
          fi
        tests:
          test_items:
            - flag: "correct"
        remediation: |
          ## Abstract
          The fencing timeout (`stonith-timeout`) determines the time Pacemaker will wait for fencing to succeed.
          The recommended values on Azure are `144` seconds for SBD only or `900` seconds when using SBD combined with the Azure Fence agent.

          ## Remediation
          Execute the following command to adjust the timeout for your usecase:  
          ```crm configure property stonith-timeout=144```  
          or  
          ```crm configure property stonith-timeout=900```

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
  - id: 1.3
    description: "SBD"
    checks:
      - id: 1.3.1
        description: "Pacemaker and SBD integration is enabled"
        audit: "cat /etc/sysconfig/sbd | grep SBD_PACEMAKER"
        tests:
          test_items:
            - flag: "SBD_PACEMAKER"
              compare:
                op: eq
                value: "yes"
        remediation: |
          ## Abstract
          For proper SBD fencing, make sure that the integration with Pacemaker is enabled.

          **IMPORTANT**: Always verify these steps in a testing environment before doing so in production ones!

          ## Remediation
          Run the following commands in order:

          1. Put cluster into maintenance mode:  
             ```crm configure property maintenance-mode=true```
          2. Stop the cluster:  
             ```crm cluster stop```
          3. Set the SBD_STARTMODE parameter to `yes` on `/etc/sysconfig/sbd`:  
             ```
             [...]
             SBD_PACEMAKER="yes"
             [...]
             ```
          4. Restart the cluster:  
             ```crm cluster start```
          5. Put cluster out of maintenance mode  
             ```crm configure property maintenance-mode=false```

          ## References
          - T.B.D.
        scored: true
      - id: 1.3.2
        description: "SBD start mode is set to always"
        audit: "cat /etc/sysconfig/sbd | grep SBD_STARTMODE"
        tests:
          test_items:
            - flag: "SBD_STARTMODE"
              compare:
                op: eq
                value: "always"
        remediation: |
          ## Abstract
          If not set to always, SBD will not automatically start if the node was previously fenced as it will expect the cluster in a clean state.

          **IMPORTANT**: Always verify these steps in a testing environment before doing so in production ones!

          ## Remediation
          Run the following commands in order:

          1. Put cluster into maintenance mode:  
             ```crm configure property maintenance-mode=true```
          2. Stop the cluster:  
             ```crm cluster stop```
          2. Set the SBD_STARTMODE parameter to `always` on `/etc/sysconfig/sbd`:  
             ```
             [...]
             SBD_STARTMODE="always"
             [...]  
             ```
          3. Restart the cluster:  
             ```crm cluster start```
          4. Put cluster out of maintenance mode:    
             ```crm configure property maintenance-mode=false```

          ## References
          - T.B.D.
        scored: true
      - id: 1.3.3
        description: "SBD service is enabled"
        audit: "systemctl is-enabled sbd"
        tests:
          test_items:
            - flag: "enabled"
        remediation: |
          ## Abstract
          If not enabled, SBD service will not start automatically after reboots, affecting the correct cluster startup.

          ## Remediation
          To enable the service, run:
          ```
          systemctl enable sbd
          ```

          ## References
          - T.B.D.
        scored: true
      - id: 1.3.4
        description: "Multiple SBD devices are configured"
        audit: |
          sbdarray=$(grep -E '^SBD_DEVICE=' /etc/sysconfig/sbd  | grep -oP 'SBD_DEVICE=\K[^.]+' | sed 's/\"//g')
          IFS=';' sbdarray=( $sbdarray )
          echo "SBD_COUNT=${#sbdarray[@]}"
        tests:
          test_items:
            - flag: "SBD_COUNT"
              compare:
                op: eq
                value: 3
        remediation: |
          ## Abstract
          It is recommended to configure 3 SBD devices for production environments.

          ## References
          -  https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#set-up-sbd-device
        scored: false
      - id: 1.3.5
        description: "SBD Timing parameters configured properly"
        audit: |
          DEF_WDTIMEOUT=60
          DEF_MSGWAIT=120
          result_wdtimeout=${DEF_TIMEOUT}
          result_msgwait=${DEF_MSGWAIT}

          sbdarray=$(grep -E '^SBD_DEVICE=' /etc/sysconfig/sbd  | grep -oP 'SBD_DEVICE=\K[^.]+' | sed 's/\"//g')
          IFS=';' sbdarray=( $sbdarray )

          for i in "${sbdarray[@]}"
          do
            msgwait=$(/usr/sbin/sbd -d ${i} dump | grep -oP 'Timeout \(msgwait\)  *: \K\d+')
            wdtimeout=$(/usr/sbin/sbd -d ${i} dump | grep -oP 'Timeout \(watchdog\)  *: \K\d+')

            if [[ "${msgwait}" -ne "${DEF_MSGWAIT}" ]]; then
              result_msgwait="${msgwait}"
            fi

            if [[ "${wdtimeout}" -ne "${DEF_WDTIMEOUT}" ]]; then
              result_wdtimeout="${wdtimeout}"
            fi
          done
          echo "${result_wdtimeout}"
          echo "${result_msgwait}"
        tests:
          test_items:
            - flag: "60"
            - flag: "120"
        remediation: |          
          ## Remediation
          Make sure you configure your SBD Watchdog Timeout to 60s and the SBD msgwait to 120s as recommended on the best practices.
          
          ## References
          -  https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#set-up-sbd-device
        scored: true
  - id: 1.4
    description: "Azure Fence Agent Checks"
    checks:
      - id: 1.4.1
        description: "Azure Fence Agent retries and timeout parameters check (in case its configured)"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/@type" > /dev/null 2>&1; then
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='pcmk_monitor_retries']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='pcmk_action_limit']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='power_timeout']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='pcmk_reboot_timeout']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/operations/op [@name='monitor']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/operations/op [@name='monitor']" | grep -oP 'timeout="\K[^"]+'
          else
            # Azure Fence Agent is not configured. We echo expected values just to fulfill the check
            echo 4
            echo 3
            echo 240
            echo 900
            echo 3600
            echo 120
          fi

        tests:
          bin_op: and
          test_items:
            - flag: 4
            - flag: 3
            - flag: 240
            - flag: 900
            - flag: 3600
            - flag: 120
        remediation: |
          ## Abstract
          The Azure SAP Clustering best practices determines specific timing and retries parameter values for proper functioning of the Azure Fence agent.
          
          ## Remediation
          Please, make sure to set the following values for the fence-agent parameter:
          ```
          pcmk_monitor_retries=4
          pcmk_action_limit=3
          power_timeout=240
          pcmk_reboot_timeout=900
          ```
          
          and the following monitor operation configuration:
          ```
          interval=3600
          timeout=120
          ```
          
          ## References
          -  https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#1-create-the-stonith-devices
        scored: false
      - id: 1.4.2
        description: "Concurrent fencing should be enabled if Azure Fence is configured"
        audit: |
          cf_status=$(crm_attribute -t crm_config -G -n concurrent-fencing --quiet)

          if cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/@type" > /dev/null 2>&1; then
            if [[ ${cf_status} = "true" ]]; then
              echo "correct"
            else
              echo "${cf_status}"
            fi
          else
            echo "fence_azure_arm not configured"
          fi
        tests:
          bin_op: or
          test_items:
            - flag: "correct"
            - flag: "fence_azure_arm not configured"
        remediation: |
          ## Abstract
          Concurrent fencing must be enabled in order to use Azure Fence agent combined with SBD fencing.

          ## Remediation
          To enable it, run:
          ```
          crm configure property concurrent-fencing=true
          ```
          
          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#1-create-the-stonith-devices
        scored: true
  - id: 1.5
    description: "SAP HANA System Replication Resource Agent Checks - Performance Optimized"
    checks:
      - id: 1.5.1
        description: "Cluster default resource-stickiness and migration-threshold properly configured"
        audit: |
          crm_attribute -t rsc_defaults -G -n resource-stickiness --quiet
          crm_attribute -t rsc_defaults -G -n migration-threshold --quiet
        tests:
          bin_op: and
          test_items:
            - flag: 1000
            - flag: 5000
        remediation: |
          ## Abstract
          For proper SAP HANA cluster actions calculations, it is needed to set resource-stickiness=1000 and migration-threshold=5000.

          ## Remediation
          Execute the following command to enable it:
          ```
          crm configure rsc_defaults resource-stickiness=1000
          crm configure rsc_defaults migration-threshold=5000
          ```
          
          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability
        scored: true
      - id: 1.5.2
        description: "SAP HANA Topology Resource Agent properly configured"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/@type" > /dev/null 2>&1; then

            is_clone=$(cibadmin -Q --xpath "//clone/primitive[@type='SAPHanaTopology']/@type" > /dev/null 2>&1; echo $?)

            if [[ "${is_clone}" = "0" ]]; then
              echo "is_clone"
            else
              echo "not_clone"
            fi

            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='monitor']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='monitor']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='start']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='start']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='stop']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHanaTopology']/operations/op [@name='stop']" | grep -oP 'timeout="\K[^"]+'
          else
            # SAPHanaTopology is not configured
            echo "SAPHanaTopology is not configured"
          fi
        tests:
          bin_op: and
          test_items:
            - flag: "is_clone"
            - flag: 10
            - flag: 600
            - flag: 0
            - flag: 600
            - flag: 0
            - flag: 300
        remediation: |
          The SAPHanaTopology resource-agent is responsible to monitor the status of the SAP HANA System Replication and provide this information for actions calculations by Pacemaker and the SAPHanaSR resource-agent

          Make sure that it is configured as a clone resource, so it runs on all nodes of the cluster and that the following timings are set for the operations:

            monitor: interval="10" timeout="600"
            start:  interval="0" timeout="600"
            stop: interval="0" timeout="300"

          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#create-sap-hana-cluster-resources
        scored: true
      - id: 1.5.3
        description: "SAP HANA Resource Agent properly configured"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='SAPHana']/@type" > /dev/null 2>&1; then

            is_msl=$(cibadmin -Q --xpath "//master/primitive[@type='SAPHana']/@type" > /dev/null 2>&1; echo $?)

            if [[ "${is_msl}" = "0" ]]; then
              echo "is_msl"
            else
              echo "not_msl"
            fi

            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='monitor'] [@role='Master']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='monitor'] [@role='Master']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='monitor'] [@role='Slave']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='monitor'] [@role='Slave']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='start']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='start']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='stop']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='stop']" | grep -oP 'timeout="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='promote']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/operations/op [@name='promote']" | grep -oP 'timeout="\K[^"]+'

            cibadmin -Q --xpath "//primitive[@type='SAPHana']/instance_attributes/nvpair [@name='PREFER_SITE_TAKEOVER']" | grep -oP 'value="\K[^"]+' | tr '[:upper:]' '[:lower:]'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/instance_attributes/nvpair [@name='AUTOMATED_REGISTER']" | grep -oP 'value="\K[^"]+' | tr '[:upper:]' '[:lower:]'
            cibadmin -Q --xpath "//primitive[@type='SAPHana']/instance_attributes/nvpair [@name='DUPLICATE_PRIMARY_TIMEOUT']" | grep -oP 'value="\K[^"]+'
          else
            # SAPHana is not configured
            echo "SAPHana RA is not configured"
          fi
        tests:
          bin_op: and
          test_items:
            - flag: "is_msl"
            - flag: 60
            - flag: 700
            - flag: 61
            - flag: 700
            - flag: 0
            - flag: 3600
            - flag: 0
            - flag: 3600
            - flag: 0
            - flag: 3600
            - flag: "true"
            - flag: "false"
            - flag: 7200
        remediation: |
          The SAPHana resource-agent is responsible to perform SAP HANA actions like start, stop and take-over based on the Pacemaker calculated transitions.

          Make sure that it is configured as a Promoted/Demoted (Master/Slave) resource, and that the following timings are set for the operations:

            monitor (master role): interval="60" timeout="700"
            monitor (slave role): interval="61" timeout="700"
            start:  interval="0" timeout="3600"
            stop: interval="0" timeout="3600"
            promote: interval="0" timeout="3600"

          And that the following input parameters are properly set:

            PREFER_SITE_TAKEOVER="true"
            DUPLICATE_PRIMARY_TIMEOUT="7200"
            AUTOMATED_REGISTER="false"

          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#create-sap-hana-cluster-resources
        scored: true
      - id: 1.5.4
        description: "Virtual IP Resource Agent properly configured"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='IPaddr2']/@type" > /dev/null 2>&1; then

            is_grouped=$(cibadmin -Q --xpath "//group/primitive[@type='IPaddr2']/@type" > /dev/null 2>&1; echo $?)

            if [[ "${is_grouped}" = "0" ]]; then
              echo "is_grouped"
            else
              echo "not_grouped"
            fi

            cibadmin -Q --xpath "//primitive[@type='IPaddr2']/operations/op [@name='monitor']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='IPaddr2']/operations/op [@name='monitor']" | grep -oP 'timeout="\K[^"]+'
          else
            # Virtual IP is not configured
            echo "vIP RA is not configured"
          fi
        tests:
          bin_op: and
          test_items:
            - flag: "is_grouped"
            - flag: 10
            - flag: 20
        remediation: |
          The IPaddr2 resource-agent is responsible to manage the floating virtual IP used to acccess the SAP HANA and to reconfigure it in case of take-overs.

          Make sure that it is configured and grouped together with the azure-lb resource agent, and that the following timings are set for the operations:

            monitor: interval="10" timeout="20"

          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#create-sap-hana-cluster-resources
        scored: true
      - id: 1.5.5
        description: "Azure Load Balancer Resource Agent properly configured"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='azure-lb']/@type" > /dev/null 2>&1; then

            is_grouped=$(cibadmin -Q --xpath "//group/primitive[@type='azure-lb']/@type" > /dev/null 2>&1; echo $?)

            if [[ "${is_grouped}" = "0" ]]; then
              echo "is_grouped"
            else
              echo "not_grouped"
            fi

            cibadmin -Q --xpath "//primitive[@type='azure-lb']/meta_attributes/nvpair [@name='resource-stickiness']" | grep -oP 'value="\K[^"]+'

          else
            # azure-lb RA is not configured
            echo "azure-lb RA is not configured"
          fi
        tests:
          bin_op: and
          test_items:
            - flag: "is_grouped"
            - flag: 0
        remediation: |
          The azure-lb resource-agent is responsible to provide health probes to inform the Azure Load Balancer if a switch to secorary node is needed.

          Make sure that it is configured and grouped together with the IPaddr2 resource and that the resource-stickiness is set to 0.
          This configuration guarantees that the cluster migrate the azure-lb together with the SAP HANA primaru resource in case of a failover.

          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#create-sap-hana-cluster-resources
        scored: true
      - id: 1.5.6
        description: "Colocation and Order Constraints properly configured"
        audit: |
          if cibadmin -Q --xpath "//constraints/rsc_colocation" > /dev/null 2>&1; then
            cibadmin -Q --xpath "//constraints/rsc_colocation" | grep -oP 'score="\K[^"]+'
            cibadmin -Q --xpath "//constraints/rsc_colocation" | grep -oP 'rsc="\K[^"]+' | grep -o g_ip
            cibadmin -Q --xpath "//constraints/rsc_colocation" | grep -oP 'rsc-role="\K[^"]+'
            cibadmin -Q --xpath "//constraints/rsc_colocation" | grep -oP 'with-rsc="\K[^"]+' | grep -o msl_SAPHana
            cibadmin -Q --xpath "//constraints/rsc_colocation" | grep -oP 'with-rsc-role="\K[^"]+'
          else
            # Colocation constraint is not configured
            echo "colocation constraint is not configured"
          fi
          if cibadmin -Q --xpath "//constraints/rsc_order" > /dev/null 2>&1; then
            cibadmin -Q --xpath "//constraints/rsc_order" | grep -oP 'kind="\K[^"]+'
            cibadmin -Q --xpath "//constraints/rsc_order" | grep -oP 'first="\K[^"]+' | grep -o cln_SAPHanaTopology
            cibadmin -Q --xpath "//constraints/rsc_order" | grep -oP 'then="\K[^"]+' | grep -o msl_SAPHana
          else
            # Colocation constraint is not configured
            echo "order constraint is not configured"
          fi
        tests:
          bin_op: and
          test_items:
            - flag: 4000
            - flag: "g_ip"
            - flag: "Started"
            - flag: "msl_SAPHana"
            - flag: "Master"
            - flag: "Optional"
            - flag: "cln_SAPHanaTopology"
            - flag: "msl_SAPHana"
        remediation: |
          An constraint enforcing the group of resources containing the vIP and azurel-lb resources to stay always together with the SAP HANA resource is needed to guarantee that the SAP HANA database will still be accessible in case of failover.
          Also, make sure that the constraing score is set to 4000 to guarantee proper actions calculations related with the defaults set for resource-stickiness=1000.

          An order constraint is also needed to guarantee that SAPHanaTopology resource starts before the SAPHana resource, avoiding miscalculations regarding the cluster status.

          IMPORTANT: This check is based on the resource prefix names recommended on the documentation (e.g. "g_ip_*", "msl_SAPHana_*", "cln_SAPHanaTopology_*").
          Please, check the naming convention in case the resource configuration is done correctly.

          More information at https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#create-sap-hana-cluster-resources
        scored: true
      - id: 1.5.7
        description: "Check SAPHanaSR configuration"
        audit: |
         sid=$(crm configure show | grep -m1 SID= | sed -e "s/.*SID=\(...\).*/\1/")
         cat /hana/shared/$sid/global/hdb/custom/config/global.ini
         sed -n -e '/ha_dr_provider_SAPHanaSR\]/,/\[/ p; /trace\]/,/\[/ p' /hana/shared/$sid/global/hdb/custom/config/global.ini
         path=$(cat /hana/shared/$sid/global/hdb/custom/config/global.ini | grep "path =" | sed "s/.*= //")
         if [ -s "$path/SAPHanaSR.py" ]; then echo "HookFound"; else echo "HookNotFound"; fi
        tests:
          test_items:
          - flag: "provider = SAPHanaSR"
          - flag: "path = /usr/share/SAPHanaSR"
          - flag: "execution_order = 1"
          - flag: "ha_dr_saphanasr = info"
          - flag: "HookFound"
        remediation: |
          ## Remediation
          Set the recommended values in global.ini.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/sap-hana-high-availability#implement-the-python-system-replication-hook-saphanasr
        scored: true
  - id: 1.6
    description: "Check that <sidadm> sudo user is set up in /etc/sudoers"
    checks:
      - id: 1.6.1
        description: "Basic sudoers entry to allow <sidadm> to use the srHook."
        audit: 'cat /etc/sudoers | grep -E ".+ +ALL=\(ALL\) +NOPASSWD: +/usr/sbin/crm_attribute +-n +hana_.+_site_srHook_\*" | sed "s/.*/true/"'
        tests:
          test_items:
          - flag: "true"
        remediation: |
          Set up the Basic sudoers entry to allow <sidadm> in /etc/sudoers to use the srHook.
          Ref: section 8.3 in https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/
        scored: true
# TO-DO: group all of these in 1.6.1
#      - id: 1.6.2
#        description: "Check SOK_SITEA alias"
#        audit: 'cat /etc/sudoers | grep -E "^Cmnd_Alias +SOK_SITEA += +/usr/sbin/crm_attribute" | grep -E "\-n +hana_.+_site_srHook_.+" | grep -E "\-v +SOK" | grep -E "\-t +crm_config" | grep -E "\-s +SAPHanaSR" | sed "s/.*/true/"'
#        tests:
#          test_items:
#          - flag: "true"
#        remediation: |
#          Set up SOK_SITEA in /etc/sudoers.
#          Ref: section 8.3 in https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/
#        scored: true
#      - id: 1.6.3
#        description: "Check SFAIL_SITEA alias"
#        audit: 'cat /etc/sudoers | grep -E "^Cmnd_Alias +SFAIL_SITEA += +/usr/sbin/crm_attribute" | grep -E "\-n +hana_.+_site_srHook_.+" | grep -E "\-v +SFAIL" | grep -E "\-t +crm_config" | grep -E "\-s +SAPHanaSR" | sed "s/.*/true/"'
#        tests:
#          test_items:
#          - flag: "true"
#        remediation: |
#          Set up SFAIL_SITEA in /etc/sudoers.
#          Ref: section 8.3 in https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/
#        scored: true
#      - id: 1.6.4
#        description: "Check SOK_SITEB alias"
#        audit: 'cat /etc/sudoers | grep -E "^Cmnd_Alias +SOK_SITEB += +/usr/sbin/crm_attribute" | grep -E "\-n +hana_.+_site_srHook_.+" | grep -E "\-v +SOK" | grep -E "\-t +crm_config" | grep -E "\-s +SAPHanaSR" | sed "s/.*/true/"'
#        tests:
#          test_items:
#          - flag: "true"
#        remediation: |
#          Set up SOK_SITEB in /etc/sudoers.
#          Ref: section 8.3 in https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/
#        scored: true
#      - id: 1.6.5
#        description: "Check SFAIL_SITEB alias"
#        audit: 'cat /etc/sudoers | grep -E "^Cmnd_Alias +SFAIL_SITEB += +/usr/sbin/crm_attribute" | grep -E "\-n +hana_.+_site_srHook_.+" | grep -E "\-v +SFAIL" | grep -E "\-t +crm_config" | grep -E "\-s +SAPHanaSR" | sed "s/.*/true/"'
#        tests:
#          test_items:
#          - flag: "true"
#        remediation: |
#          Set up SFAIL_SITEB in /etc/sudoers.
#          Ref: section 8.3 in https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/
#        scored: true
#      - id: 1.6.6
#        description: "Check <sidadm> ALL=(ALL) NOPASSWD"
#        audit: 'cat /etc/sudoers | grep -E ".+ +ALL=\(ALL\) +NOPASSWD:" | sed "s/.*: //" | grep SOK_SITEA | grep SFAIL_SITEA | grep SOK_SITEB | grep SFAIL_SITEB | sed "s/.*/true/"'
#        tests:
#          test_items:
#          - flag: "true"
#        remediation: |
#          Set up the <sidadm> ALL=(ALL) NOPASSWD in /etc/sudoers.
#          Ref: section 8.3 in https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/
#        scored: true
