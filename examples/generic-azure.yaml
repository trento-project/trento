
controls:
version: 1.0.0
id: 1
description: "HA Configuration checks for SAP on MS Azure (generic scenario)"
type: "master"
groups:
  - id: 1.1
    description: "Corosync"
    checks:
      - id: 1.1.1
        description: "Corosync Token is set to 30000"
        audit: 'grep -E "^\s*token:\s*30000" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 30000
        remediation: |
          ## Remediation
          Adjust the Corosync `token` timeout as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.1.runtime
        description: "Corosync Token is set to 30000 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.token (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 30000
        remediation: |
          ## Abstract
          The runtime value of the Corosync `token` timeout is not set as recommended.

          ## Remediation
          Adjust the corosync `token` timeout as recommended on the Azure best practices, and reload the corosync service

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.2
        description: "Corosync consensus is set to 36000"
        audit: 'grep -E "^\s*consensus:\s*36000" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 36000
        remediation: |
          ## Remediation
          Adjust the Corosync `consensus` timeout as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.2.runtime
        description: "Corosync consensus is set to 36000 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.consensus (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 36000
        remediation: |
          ## Abstract
          The runtime value of the Corosync `consensus` timeout is not set as recommended.

          ## Remediation
          Adjust the corosync `consensus` timeout as recommended on the Azure best practices, and reload the corosync service.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.3
        description: "Corosync max_messages is set to 20"
        audit: 'grep -E "^\s*max_messages:\s*20" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 20
        remediation: |
          ## Remediation
          Adjust the Corosync `max_messages` parameter as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.3.runtime
        description: "Corosync max_messages is set to 20 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.max_messages (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 20
        remediation: |
          ## Abstract
          The runtime value of the Corosync `max_messages` parameter is not set as recommended.

          ## Remediation
          Adjust the corosync `max_messages` parameter as recommended on the Azure best practices, and reload the corosync service.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.4
        description: "Corosync join parameter is set to 60"
        audit: 'grep -E "^\s*join:\s*60" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 60
        remediation: |
          ## Abstract
          The `max_messages` Corosync configuration parameter is not set as recommended.

          ## Remediation
          Adjust the Corosync `join` parameter as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.4.runtime
        description: "Corosync join parameter is set to 60 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.join (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 60
        remediation: |
          ## Abstract
          The runtime value of the Corosync `join` parameter is not set as recommended.

          ## Remediation
          Adjust the corosync `join` parameter as recommended on the Azure best practices, and reload the corosync service.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.5
        description: "Corosync token_retransmits_before_loss_const parameter is set to 10"
        audit: 'grep -E "^\s*token_retransmits_before_loss_const:\s*10" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 10
        remediation: |
          ## Remediation
          Adjust the corosync `token_retransmits_before_loss_const` parameter as recommended on the Azure best practices.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.5.runtime
        description: "Corosync token_retransmits_before_loss_const parameter is set to 10 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.config.totem.token_retransmits_before_loss_const (u32) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 10
        remediation: |
          ## Abstract
          The runtime value of the corosync `token_retransmits_before_loss_const` parameter is not set as recommended

          ## Remediation
          Adjust the corosync `token_retransmits_before_loss_const` parameter as recommended on the Azure best practices, and reload the corosync service.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.6
        description: "Corosync transport parameter is set to udpu"
        audit: 'grep -E "^\s*transport:\s*udpu" /etc/corosync/corosync.conf | sed "s/.*:\s*//"'
        tests:
          test_items:
            - flag: "udpu"
        remediation: |
          ## Remediation
          It will be difficult to change the corosync MCAST transport to UCAST
          on an existing cluster. Besides adding the 'transport: udpu' parameter one
          would have to configure each node separatelly. The easiest way to
          make the cluster UCAST it to bootstrap it. When creating the cluster
          you would have to add the key -u, for example
          $ crm cluster init -u ...

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.6.runtime
        description: "Corosync transport parameter is set to udpu (runtime)"
        audit: 'corosync-cmapctl | grep "totem.transport (str) = " | sed "s/.*= //"'
        tests:
          test_items:
            - flag: "udpu"
        remediation: |
          ## Remediation
          It will be difficult to change the corosync MCAST transport to UCAST
          on an existing cluster. Besides adding the 'transport: udpu' parameter one
          would have to configure each node separatelly. The easiest way to
          make the cluster UCAST it to bootstrap it. When creating the cluster
          you would have to add the key -u, for example
          $ crm cluster init -u ...

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.7
        description: "Corosync expected_votes parameter is set to 2"
        audit: 'grep -E "^\s*expected_votes:\s*2" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 2
        remediation: |
          ## Remediation
          Adjust the corosync `expected_votes` parameter to `2` to make sure pacemaker calculates the actions properly for a two-node cluster.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.8
        description: "Corosync two_node parameter is set to 1"
        audit: 'grep -E "^\s*two_node:\s*1" /etc/corosync/corosync.conf | sed "s/[^0-9]//g"'
        tests:
          test_items:
            - flag: 1
        remediation: |
          ## Abstract
          The runtime value of the corosync `two_node` parameter is not set as recommended.

          ## Remediation
          Adjust the corosync two_node parameter to `1` to make sure Pacemaker calculates the actions properly for a two-node cluster.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.8.runtime
        description: "Corosync two_node parameter is set to 1 (runtime)"
        audit: 'corosync-cmapctl | grep "runtime.votequorum.two_node (u8) = " | sed "s/^.*= //"'
        tests:
          test_items:
            - flag: 1
        remediation: |
          ## Abstract
          The runtime value of the corosync `two_node` parameter is not set as recommended.

          ## Remediation
          Adjust the corosync `two_node` parameter to `1` to make sure Pacemaker calculates the actions properly for a two-node cluster, and reload the Corosync service.

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
      - id: 1.1.9
        description: "Check there are at least 2 corosync communication rings"
        audit: 'echo rings=$(cat /etc/corosync/corosync.conf | grep interface | wc -l)'
        tests:
          test_items:
            - flag: "rings"
              compare:
                op: gt
                value: 1
        remediation: |
          ## Abstract
          It is strongly recommended to add a second ring to the corosync communication.

          ## References
          - section 9.1 in https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/
        scored: false
      - id: 1.1.9.runtime
        description: "Check there are at least 2 corosync communication rings (runtime)"
        audit: |
          corosync-cmapctl | grep totem.interface.0 | sed "s/^.*/true0/"
          corosync-cmapctl | grep totem.interface.1 | sed "s/^.*/true1/"
        tests:
          bin_op: and
          test_items:
            - flag: true0
            - flag: true1
        remediation: |
          ## Abstract
          It is strongly recommended to add a second ring to the corosync communication.

          ## References
          - section 9.1 in https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/
        scored: false
  - id: 1.2
    description: "Pacemaker"
    checks:
      - id: 1.2.1
        description: "Fencing is enabled"
        audit: "crm_attribute -t crm_config -G -n stonith-enabled --quiet"
        tests:
          test_items:
            - flag: 'true'
        remediation: |
          ## Abstract
          Fencing is mandatory to guarantee data integrity for your SAP Applications.
          Running a HA Cluster without fencing is not supported and might cause data loss.

          ## Remediation
          Execute the following command to enable it:
          ```
          crm configure property stonith-enabled=true
          ```

          ## References
          - T.B.D.
        scored: true
      - id: 1.2.2
        description: "Fencing timeout is correctly configured"
        audit: |
          timeout=$(crm_attribute -t crm_config -G -n stonith-timeout --quiet)
          if cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/@type" > /dev/null 2>&1; then
            if [[ ${timeout} = "900" ]]; then
              echo "correct"
            else
              echo ${timeout}
            fi
          elif [[ ${timeout} = "144" ]]; then
            echo "correct"
          else
            echo ${timeout}
          fi
        tests:
          test_items:
            - flag: "correct"
        remediation: |
          ## Abstract
          The fencing timeout (`stonith-timeout`) determines the time Pacemaker will wait for fencing to succeed.
          The recommended values on Azure are `144` seconds for SBD only or `900` seconds when using SBD combined with the Azure Fence agent.

          ## Remediation
          Execute the following command to adjust the timeout for your usecase:
          ```crm configure property stonith-timeout=144```
          or
          ```crm configure property stonith-timeout=900```

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: true
  - id: 1.3
    description: "SBD"
    checks:
      - id: 1.3.1
        description: "Pacemaker and SBD integration is enabled"
        audit: "cat /etc/sysconfig/sbd | grep SBD_PACEMAKER"
        tests:
          test_items:
            - flag: "SBD_PACEMAKER"
              compare:
                op: eq
                value: "yes"
        remediation: |
          ## Abstract
          For proper SBD fencing, make sure that the integration with Pacemaker is enabled.

          **IMPORTANT**: Always verify these steps in a testing environment before doing so in production ones!

          ## Remediation
          Run the following commands in order:

          1. Put cluster into maintenance mode:
             ```crm configure property maintenance-mode=true```
          2. Stop the cluster:
             ```crm cluster stop```
          3. Set the SBD_STARTMODE parameter to `yes` on `/etc/sysconfig/sbd`:
             ```
             [...]
             SBD_PACEMAKER="yes"
             [...]
             ```
          4. Restart the cluster:
             ```crm cluster start```
          5. Put cluster out of maintenance mode
             ```crm configure property maintenance-mode=false```

          ## References
          - T.B.D.
        scored: true
      - id: 1.3.2
        description: "SBD start mode is set to always"
        audit: "cat /etc/sysconfig/sbd | grep SBD_STARTMODE"
        tests:
          test_items:
            - flag: "SBD_STARTMODE"
              compare:
                op: eq
                value: "always"
        remediation: |
          ## Abstract
          If not set to always, SBD will not automatically start if the node was previously fenced as it will expect the cluster in a clean state.

          **IMPORTANT**: Always verify these steps in a testing environment before doing so in production ones!

          ## Remediation
          Run the following commands in order:

          1. Put cluster into maintenance mode:
             ```crm configure property maintenance-mode=true```
          2. Stop the cluster:
             ```crm cluster stop```
          2. Set the SBD_STARTMODE parameter to `always` on `/etc/sysconfig/sbd`:
             ```
             [...]
             SBD_STARTMODE="always"
             [...]
             ```
          3. Restart the cluster:
             ```crm cluster start```
          4. Put cluster out of maintenance mode:
             ```crm configure property maintenance-mode=false```

          ## References
          - T.B.D.
        scored: true
      - id: 1.3.3
        description: "SBD service is enabled"
        audit: "systemctl is-enabled sbd"
        tests:
          test_items:
            - flag: "enabled"
        remediation: |
          ## Abstract
          If not enabled, SBD service will not start automatically after reboots, affecting the correct cluster startup.

          ## Remediation
          To enable the service, run:
          ```
          systemctl enable sbd
          ```

          ## References
          - T.B.D.
        scored: true
      - id: 1.3.4
        description: "Multiple SBD devices are configured"
        audit: |
          sbdarray=$(grep -E '^SBD_DEVICE=' /etc/sysconfig/sbd  | grep -oP 'SBD_DEVICE=\K[^.]+' | sed 's/\"//g')
          IFS=';' sbdarray=( $sbdarray )
          echo "SBD_COUNT=${#sbdarray[@]}"
        tests:
          test_items:
            - flag: "SBD_COUNT"
              compare:
                op: eq
                value: 3
        remediation: |
          ## Abstract
          It is recommended to configure 3 SBD devices for production environments.

          ## References
          -  https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#set-up-sbd-device
        scored: false
      - id: 1.3.5
        description: "Watchdog timeout is 60 sec"
        audit: |
          DEF_WDTIMEOUT=60
          result_wdtimeout=${DEF_WDTIMEOUT}

          sbdarray=$(grep -E '^SBD_DEVICE=' /etc/sysconfig/sbd  | grep -oP 'SBD_DEVICE=\K[^.]+' | sed 's/\"//g')
          IFS=';' sbdarray=( $sbdarray )

          for i in "${sbdarray[@]}"
          do
            wdtimeout=$(/usr/sbin/sbd -d ${i} dump | grep -oP 'Timeout \(watchdog\)  *: \K\d+')

            if [[ "${wdtimeout}" -ne "${DEF_WDTIMEOUT}" ]]; then
              result_wdtimeout="${wdtimeout}"
            fi
          done
          echo "${result_wdtimeout}"
        tests:
          test_items:
            - flag: "60"
        remediation: |
          ## Remediation
          Make sure you configure your SBD Watchdog Timeout to 60s as recommended on the best practices.

          ## References
          -  https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#set-up-sbd-device
        scored: true
      - id: 1.3.6
        description: "msgwait timeout is 2*(watchdog timeout)"
        audit: |
          sbdarray=$(grep -E '^SBD_DEVICE=' /etc/sysconfig/sbd  | grep -oP 'SBD_DEVICE=\K[^.]+' | sed 's/\"//g')
          IFS=';' sbdarray=( $sbdarray )

          for i in "${sbdarray[@]}"
          do
            msgwait=$(/usr/sbin/sbd -d ${i} dump | grep -oP 'Timeout \(msgwait\)  *: \K\d+')
            wdtimeout=$(/usr/sbin/sbd -d ${i} dump | grep -oP 'Timeout \(watchdog\)  *: \K\d+')

            if [[ "${msgwait}" -ne "$(( ${wdtimeout}*2 ))" ]]; then
              echo false
              break
            fi
          done
          echo true
        tests:
          test_items:
            - flag: "true"
        remediation: |
          ## Remediation
          Make sure you configure your the SBD msgwait to 2 * (SBD Watchdog Timeout) as recommended on the best practices.

          ## References
          -  https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#set-up-sbd-device
        scored: true
      - id: 1.3.7
        description: "2-nodes cluster must either have disk-based sbd or qdevice"
        audit: |
          if [[ $(crm_node -l | wc -l) != "2" ]]; then
            echo passed
            exit 0
          fi

          sbdarray=$(grep -E '^SBD_DEVICE=' /etc/sysconfig/sbd  | grep -oP 'SBD_DEVICE=\K[^.]+' | sed 's/\"//g')
          IFS=';' sbdarray=( $sbdarray )
          device_count=${#sbdarray[@]}

          # If there is at least 1 device and there is an sbd device used by pacemaker
          if [[ $device_count != "0" ]] && crm conf show | grep "stonith:external/sbd"; then
            echo passed
            exit 0
          fi

          # If the qdevice is configured it's also good
          if corosync-quorumtool | tail -n1 | grep -i qdevice; then
            echo passed
            exit 0
          fi
          echo failed
        tests:
          test_items:
            - flag: passed
        remediation: |
          ## Remediation
          HA cluster with 2 nodes must either have a disk-based SBD or a Qdevice.

          ## References
          - section 2 in https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/
        scored: true
  - id: 1.4
    description: "Azure Fence Agent Checks"
    checks:
      - id: 1.4.1
        description: "Azure Fence Agent retries and timeout parameters check (in case its configured)"
        audit: |
          if cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/@type" > /dev/null 2>&1; then
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='pcmk_monitor_retries']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='pcmk_action_limit']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='power_timeout']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/instance_attributes/nvpair[@name='pcmk_reboot_timeout']" | grep -oP 'value="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/operations/op [@name='monitor']" | grep -oP 'interval="\K[^"]+'
            cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/operations/op [@name='monitor']" | grep -oP 'timeout="\K[^"]+'
          else
            # Azure Fence Agent is not configured. We echo expected values just to fulfill the check
            echo 4
            echo 3
            echo 240
            echo 900
            echo 3600
            echo 120
          fi

        tests:
          bin_op: and
          test_items:
            - flag: 4
            - flag: 3
            - flag: 240
            - flag: 900
            - flag: 3600
            - flag: 120
        remediation: |
          ## Abstract
          The Azure SAP Clustering best practices determines specific timing and retries parameter values for proper functioning of the Azure Fence agent.

          ## Remediation
          Please, make sure to set the following values for the fence-agent parameter:
          ```
          pcmk_monitor_retries=4
          pcmk_action_limit=3
          power_timeout=240
          pcmk_reboot_timeout=900
          ```

          and the following monitor operation configuration:
          ```
          interval=3600
          timeout=120
          ```

          ## References
          -  https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#1-create-the-stonith-devices
        scored: false
      - id: 1.4.2
        description: "Concurrent fencing should be enabled if Azure Fence is configured"
        audit: |
          cf_status=$(crm_attribute -t crm_config -G -n concurrent-fencing --quiet)

          if cibadmin -Q --xpath "//primitive[@type='fence_azure_arm']/@type" > /dev/null 2>&1; then
            if [[ ${cf_status} = "true" ]]; then
              echo "correct"
            else
              echo "${cf_status}"
            fi
          else
            echo "fence_azure_arm not configured"
          fi
        tests:
          bin_op: or
          test_items:
            - flag: "correct"
            - flag: "fence_azure_arm not configured"
        remediation: |
          ## Abstract
          Concurrent fencing must be enabled in order to use Azure Fence agent combined with SBD fencing.

          ## Remediation
          To enable it, run:
          ```
          crm configure property concurrent-fencing=true
          ```

          ## References
          - https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker#1-create-the-stonith-devices
        scored: true
  - id: 1.5
    description: "Miscellaneous checks"
    checks:
      - id: 1.5.1
        description: "/etc/hosts contains all cluster nodes"
        audit: |
          for i in $(crm_node -l | cut -d' ' -f2); do
            if ! cat /etc/hosts |grep $i; then echo found=false; fi
          done
        tests:
          test_items:
            - flag: "found"
              compare:
                op: noteq
                value: false
        remediation: |
          ## Abstract
          If using host names in the cluster configuration, it is vital to have reliable host name resolution. The cluster communication will fail, if the names are not available and that can lead to cluster failover delays. The benefit of using /etc/hosts is that your cluster becomes independent of DNS, which could be a single point of failures too.Name resolution of cluster nodes and virtual IPs is recommended to be local.

          ## Remedation
          Specify IP addresses of all cluster nodes in the /etc/hosts.

          ## References
          - Cluster installation in https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/sap/high-availability-guide-suse-pacemaker
        scored: false
      - id: 1.5.2
        description: "hacluster password is changed from default 'linux'"
        audit: |
          function attempt_login {
           /usr/bin/expect << 'EOF'
           set timeout 5
           spawn ssh -o "StrictHostKeyChecking no" hacluster@127.0.0.1 :
           expect "assword:"
           send -- "linux\r"
           expect {
             "assword:" { exit 1}
             eof { exit 0 }
           }

           foreach {pid spawnid os_error_flag value} [wait] break
           exit $value
          EOF
          return $?
          }
          # The test fails if we could successfully log into hacluster
          if attempt_login; then echo passed; else echo blocked; fi
        tests:
          test_items:
            - flag: blocked
        remediation: |
          ## Abstract
          The password of the hacluster user should be changed after setting
          up the clsuter

          ## Remediation
          ```sudo passwd hacluster```

          ## References
          - section 9.1.2 https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/
      - id: 1.5.3
        description: "ntp clock synchronization is working and using the slew mode"
        audit: |
          # The third field of ntpq -p is the stratum and if it's 16
          # the sync is disabled. Make sure it's not 16
          if ! sudo ntpq -pn | grep "No association ID's returned" \
          && sudo ntpq -pn | awk '{print $1 " " $3}' | grep -e '^\*' | awk '{print $2}' | grep -e "[0-9]" | grep -v 16; then
            if ps aux | grep "/usr/sbin/[n]tpd" | grep "\-x"; then echo passed; fi
          fi
        tests:
          test_items:
            - flag: "passed"
        remediation: |
          ## Abstract
          Check that ntp clock synchronization is working and the "slew" mode
          is enabled.

          ## Remediation
          If you only want to set-up NTP clients execute the following commands:
          ```
          zypper in -y ntp
          echo 'server <server-ip-or-name> iburst' >> /etc/ntp.conf
          systemctl restart ntpd
          ntpq -p
          ```
          Where the server-ip-or-name is the name or ip of an NTP server.
          You can add many NTP servers into the /etc/ntp.conf file.

          To run the ntpd service in the "slew" mode set the -x flag for the
          NTPD_OPTIONS in the /etc/sysconfig/ntp file. For example
          ```NTPD_OPTIONS="-x -g -u ntp:ntp"``` and restart the service
          ```systemctl restart ntpd```

          ## References
          - https://launchpad.support.sap.com/#/notes/2578899.
        scored: true
